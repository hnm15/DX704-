{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md413FzAvFD8"
      },
      "source": [
        "# DX 704 Week 6 Project\n",
        "\n",
        "This project will develop a treatment plan for a fictious illness \"Twizzleflu\".\n",
        "Twizzleflu is a mild illness caused by a virus.\n",
        "The main symptoms are a mild fever, fidgeting, and kicking the blankets off the bed or couch.\n",
        "Mild dehydration has also been reported in more severe cases.\n",
        "These symptoms typically last 1-2 weeks without treatment.\n",
        "Word on the internet says that Twizzleflu can be cured faster by drinking copious orange juice, but this has not been supported by evidence so far.\n",
        "You will be provided with a theoretical model of Twizzleflu modeled as a Markov decision process.\n",
        "Based on the model, you will compute optimal treatment plans to optimize different criteria, and compare patient discomfort with the different plans."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzyRo9Tw5VcB"
      },
      "source": [
        "The full project description, a template notebook, and raw data are available on GitHub: [Project 6 Materials](https://github.com/bu-cds-dx704/dx704-project-06)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGYOZcnP6Vfu"
      },
      "source": [
        "We will model Twizzleflu as a Markov decision process.\n",
        "The model transition probabilities are provided in the file \"twizzleflu-transitions.tsv\" and the expected rewards are in \"twizzleflu-rewards.tsv\".\n",
        "The goal for Twizzleflu is to minimize the expected discomfort of the patient which is expressed as negative rewards in the file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlm2sUsades5"
      },
      "source": [
        "## Example Code\n",
        "\n",
        "You may find it helpful to refer to these GitHub repositories of Jupyter notebooks for example code.\n",
        "\n",
        "* https://github.com/bu-cds-omds/dx601-examples\n",
        "* https://github.com/bu-cds-omds/dx602-examples\n",
        "* https://github.com/bu-cds-omds/dx603-examples\n",
        "* https://github.com/bu-cds-omds/dx704-examples\n",
        "\n",
        "Any calculations demonstrated in code examples or videos may be found in these notebooks, and you are allowed to copy this example code in your homework answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8oSLkMqvMFF"
      },
      "source": [
        "## Part 1: Evaluate a Do Nothing Plan\n",
        "\n",
        "One of the treatment actions is to do nothing.\n",
        "Calculate the expected discomfort (not rewards) of a policy that always does nothing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvG4mi_sAF9A"
      },
      "source": [
        "Hint: for this value calculation and later ones, use value iteration.\n",
        "The analytical solution has difficulties in practice when there is no discount factor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "transitions_df = pd.read_csv('twizzleflu-transitions.tsv', sep='\\t')\n",
        "rewards_df = pd.read_csv('twizzleflu-rewards.tsv', sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "States: ['exposed-1', 'exposed-2', 'exposed-3', 'recovered', 'symptoms-1', 'symptoms-2', 'symptoms-3']\n",
            "Actions: ['do-nothing', 'drink-oj', 'sleep-8']\n",
            "\n",
            "Reward matrix shape: (7, 3)\n",
            "Converged in 55 iterations\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "states = sorted(transitions_df['state'].unique())\n",
        "actions = sorted(transitions_df['action'].unique())\n",
        "print(f\"\\nStates: {states}\")\n",
        "print(f\"Actions: {actions}\")\n",
        "\n",
        "T = {}\n",
        "for action in actions:\n",
        "    T[action] = np.zeros((len(states), len(states)))\n",
        "    action_data = transitions_df[transitions_df['action'] == action]\n",
        "    for _, row in action_data.iterrows():\n",
        "        s_idx = states.index(row['state'])\n",
        "        s_next_idx = states.index(row['next_state'])\n",
        "        T[action][s_idx, s_next_idx] = row['probability']\n",
        "\n",
        "R = np.zeros((len(states), len(actions)))\n",
        "for _, row in rewards_df.iterrows():\n",
        "    s_idx = states.index(row['state'])\n",
        "    a_idx = actions.index(row['action'])\n",
        "    R[s_idx, a_idx] = row['reward']\n",
        "\n",
        "print(\"\\nReward matrix shape:\", R.shape)\n",
        "\n",
        "\n",
        "def value_iteration_fixed_policy(T_policy, R_policy, gamma=1.0, epsilon=1e-6, max_iter=10000):\n",
        "    \"\"\"\n",
        "    Value iteration for a fixed policy (no discount factor).\n",
        "    T_policy: transition matrix for the fixed policy (n_states x n_states)\n",
        "    R_policy: reward vector for the fixed policy (n_states,)\n",
        "    \"\"\"\n",
        "    n_states = len(R_policy)\n",
        "    V = np.zeros(n_states)\n",
        "    \n",
        "    for iteration in range(max_iter):\n",
        "        V_new = R_policy + gamma * T_policy @ V\n",
        "        \n",
        "        if np.max(np.abs(V_new - V)) < epsilon:\n",
        "            print(f\"Converged in {iteration + 1} iterations\")\n",
        "            break\n",
        "        \n",
        "        V = V_new\n",
        "    \n",
        "    return V\n",
        "\n",
        "\n",
        "do_nothing_action = 'do-nothing'  \n",
        "if do_nothing_action not in actions:\n",
        "    print(f\"\\nWarning: '{do_nothing_action}' not found. Available actions: {actions}\")\n",
        "    print(\"Using first action as 'do nothing'\")\n",
        "    do_nothing_action = actions[0]\n",
        "\n",
        "do_nothing_idx = actions.index(do_nothing_action)\n",
        "T_nothing = T[do_nothing_action]\n",
        "R_nothing = R[:, do_nothing_idx]\n",
        "\n",
        "V_nothing = value_iteration_fixed_policy(T_nothing, R_nothing)\n",
        "\n",
        "discomfort_nothing = -V_nothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oji9gHEk8ytE"
      },
      "source": [
        "Save the expected discomfort by state to a file \"do-nothing-discomfort.tsv\" with columns state and expected_discomfort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "ZLDuiAb99ACA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== PART 1: Do Nothing Discomfort ===\n",
            "        state  expected_discomfort\n",
            "0   exposed-1             3.413330\n",
            "1   exposed-2             4.266664\n",
            "2   exposed-3             5.333331\n",
            "3   recovered            -0.000000\n",
            "4  symptoms-1             6.666664\n",
            "5  symptoms-2             5.000000\n",
            "6  symptoms-3             1.666667\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "do_nothing_df = pd.DataFrame({\n",
        "    'state': states,\n",
        "    'expected_discomfort': discomfort_nothing\n",
        "})\n",
        "do_nothing_df.to_csv('do-nothing-discomfort.tsv', sep='\\t', index=False)\n",
        "print(\"\\n=== PART 1: Do Nothing Discomfort ===\")\n",
        "print(do_nothing_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-8sGANC-Dzs"
      },
      "source": [
        "Submit \"do-nothing-discomfort.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ1ietVp9BCS"
      },
      "source": [
        "## Part 2: Compute an Optimal Treatment Plan\n",
        "\n",
        "Compute an optimal treatment plan for Twizzleflu.\n",
        "It should minimize the expected discomfort (maximize the rewards)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "6fdjt6qk9mZM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converged in 66 iterations\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "def value_iteration_mdp(T, R, states, actions, gamma=1.0, epsilon=1e-6, max_iter=10000):\n",
        "    \"\"\"\n",
        "    Value iteration for MDP to find optimal policy.\n",
        "    Returns: optimal values and optimal policy\n",
        "    \"\"\"\n",
        "    n_states = len(states)\n",
        "    n_actions = len(actions)\n",
        "    V = np.zeros(n_states)\n",
        "    \n",
        "    for iteration in range(max_iter):\n",
        "        V_new = np.zeros(n_states)\n",
        "        \n",
        "        for s in range(n_states):\n",
        "            Q_values = np.zeros(n_actions)\n",
        "            for a in range(n_actions):\n",
        "                action_name = actions[a]\n",
        "                Q_values[a] = R[s, a] + gamma * T[action_name][s, :] @ V\n",
        "            \n",
        "            V_new[s] = np.max(Q_values)\n",
        "        \n",
        "        if np.max(np.abs(V_new - V)) < epsilon:\n",
        "            print(f\"Converged in {iteration + 1} iterations\")\n",
        "            break\n",
        "        \n",
        "        V = V_new\n",
        "    \n",
        "    policy = []\n",
        "    for s in range(n_states):\n",
        "        Q_values = np.zeros(n_actions)\n",
        "        for a in range(n_actions):\n",
        "            action_name = actions[a]\n",
        "            Q_values[a] = R[s, a] + gamma * T[action_name][s, :] @ V\n",
        "        \n",
        "        optimal_action_idx = np.argmax(Q_values)\n",
        "        policy.append(actions[optimal_action_idx])\n",
        "    \n",
        "    return V, policy\n",
        "\n",
        "V_optimal, policy_optimal = value_iteration_mdp(T, R, states, actions)\n",
        "discomfort_optimal = -V_optimal\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRcByl1h9nBf"
      },
      "source": [
        "Save the optimal actions for each state to a file \"minimum-discomfort-actions.tsv\" with columns state and action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "FhAajvpX9wru"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "optimal_actions_df = pd.DataFrame({\n",
        "    'state': states,\n",
        "    'action': policy_optimal\n",
        "})\n",
        "optimal_actions_df.to_csv('minimum-discomfort-actions.tsv', sep='\\t', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr00MlhL-Hdv"
      },
      "source": [
        "Submit \"minimum-discomfort-actions.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65p3NRTy9xjT"
      },
      "source": [
        "## Part 3: Expected Discomfort\n",
        "\n",
        "Using your previous optimal policy, compute the expected discomfort for each state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "t5bHbK24-AhQ"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "optimal_values_df = pd.DataFrame({\n",
        "    'state': states,\n",
        "    'expected_discomfort': discomfort_optimal\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er6-0c0f-BGw"
      },
      "source": [
        "Save your results in a file \"minimum-discomfort-values.tsv\" with columns state and expected_discomfort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "NAQFQnp_-TZ1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        state  expected_discomfort\n",
            "0   exposed-1             0.749999\n",
            "1   exposed-2             1.499999\n",
            "2   exposed-3             2.999998\n",
            "3   recovered            -0.000000\n",
            "4  symptoms-1             5.999997\n",
            "5  symptoms-2             4.500000\n",
            "6  symptoms-3             1.500000\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "optimal_values_df.to_csv('minimum-discomfort-values.tsv', sep='\\t', index=False)\n",
        "print(optimal_values_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83wnFZfk-UDd"
      },
      "source": [
        "Submit \"minimum-discomfort-values.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKUTt9gx-XBF"
      },
      "source": [
        "## Part 4: Minimizing Twizzleflu Duration\n",
        "\n",
        "Modifiy the Markov decision process to minimize the days until the Twizzle flu is over.\n",
        "To do so, change the reward function to always be -1 if the current state corresponds to being sick and 0 if the current state corresponds to being better.\n",
        "To be clear, the action does not matter for this reward function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "HXrnkCh5-trk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Warning: No 'better' state found. Assuming last state is 'better'\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "better_states = [s for s in states if 'better' in s.lower() or 'healthy' in s.lower() or 'well' in s.lower()]\n",
        "if not better_states:\n",
        "    print(\"\\nWarning: No 'better' state found. Assuming last state is 'better'\")\n",
        "    better_states = [states[-1]]\n",
        "\n",
        "R_duration = np.zeros((len(states), len(actions)))\n",
        "for s_idx, state in enumerate(states):\n",
        "    for a_idx in range(len(actions)):\n",
        "        if state in better_states:\n",
        "            R_duration[s_idx, a_idx] = 0\n",
        "        else:\n",
        "            R_duration[s_idx, a_idx] = -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je9Rt239-uRl"
      },
      "source": [
        "Save your new reward function in a file \"duration-rewards.tsv\" in the same format as \"twizzleflu-rewards.tsv\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "_cmV1ewj-4-Q"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "duration_rewards_list = []\n",
        "for s_idx, state in enumerate(states):\n",
        "    for a_idx, action in enumerate(actions):\n",
        "        duration_rewards_list.append({\n",
        "            'state': state,\n",
        "            'action': action,\n",
        "            'reward': R_duration[s_idx, a_idx]\n",
        "        })\n",
        "\n",
        "duration_rewards_df = pd.DataFrame(duration_rewards_list)\n",
        "duration_rewards_df.to_csv('duration-rewards.tsv', sep='\\t', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0lubs9v-5XQ"
      },
      "source": [
        "Submit \"duration-rewards.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf73YFzB-802"
      },
      "source": [
        "## Part 5: Optimize for Shorter Twizzleflu\n",
        "\n",
        "Compute an optimal policy to minimize the duration of Twizzleflu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "Sa_HI0f0_FHA"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "V_duration, policy_duration = value_iteration_mdp(T, R_duration, states, actions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px1xDndA_F3O"
      },
      "source": [
        "Save the optimal actions for each state to a file \"minimum-duration-actions.tsv\" with columns state and action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "PGvWqSiI_Sqy"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "duration_actions_df = pd.DataFrame({\n",
        "    'state': states,\n",
        "    'action': policy_duration\n",
        "})\n",
        "duration_actions_df.to_csv('minimum-duration-actions.tsv', sep='\\t', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itPVLMaM_UDn"
      },
      "source": [
        "Submit \"minimum-duration-actions.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOzSQ3fV_XBO"
      },
      "source": [
        "## Part 6: Shorter Twizzleflu?\n",
        "\n",
        "Compute the expected number of days sick for each state to a file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "WO_yubXg_gxn"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "expected_sick_days = -V_duration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zf8j6_D_hbZ"
      },
      "source": [
        "Save the expected sick days for each state to a file \"minimum-duration-days.tsv\" with columns state and expected_sick_days."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "yWS2HNVl_o3P"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "duration_days_df = pd.DataFrame({\n",
        "    'state': states,\n",
        "    'expected_sick_days': expected_sick_days\n",
        "})\n",
        "duration_days_df.to_csv('minimum-duration-days.tsv', sep='\\t', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVApozXF_pjI"
      },
      "source": [
        "Submit \"minimum-duration-days.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Znw87KK7_uv5"
      },
      "source": [
        "## Part 7: Speed vs Pampering\n",
        "\n",
        "Compute the expected discomfort using the policy to minimize days sick, and compare the results to the expected discomfort when optimizing to minimize discomfort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "0AdnpD-6__y5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converged in 73 iterations\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "T_duration_policy = np.zeros((len(states), len(states)))\n",
        "R_duration_policy = np.zeros(len(states))\n",
        "\n",
        "for s_idx, state in enumerate(states):\n",
        "    action = policy_duration[s_idx]\n",
        "    T_duration_policy[s_idx, :] = T[action][s_idx, :]\n",
        "    a_idx = actions.index(action)\n",
        "    R_duration_policy[s_idx] = R[s_idx, a_idx]\n",
        "\n",
        "V_duration_discomfort = value_iteration_fixed_policy(T_duration_policy, R_duration_policy)\n",
        "discomfort_speed = -V_duration_discomfort\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3ZVJ2lcAAkP"
      },
      "source": [
        "Save the results to a file \"policy-comparison.tsv\" with columns state, speed_discomfort, and minimize_discomfort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "9H9EG0zTAMt1"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'state': states,\n",
        "    'speed_discomfort': discomfort_speed,\n",
        "    'minimize_discomfort': discomfort_optimal\n",
        "})\n",
        "comparison_df.to_csv('policy-comparison.tsv', sep='\\t', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVhLZuuaANNf"
      },
      "source": [
        "Submit \"policy-comparison.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smsTLuFcvR-I"
      },
      "source": [
        "## Part 8: Code\n",
        "\n",
        "Please submit a Jupyter notebook that can reproduce all your calculations and recreate the previously submitted files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi8lV2pbvWMs"
      },
      "source": [
        "## Part 9: Acknowledgements\n",
        "\n",
        "If you discussed this assignment with anyone, please acknowledge them here.\n",
        "If you did this assignment completely on your own, simply write none below.\n",
        "\n",
        "If you used any libraries not mentioned in this module's content, please list them with a brief explanation what you used them for. If you did not use any other libraries, simply write none below.\n",
        "\n",
        "If you used any generative AI tools, please add links to your transcripts below, and any other information that you feel is necessary to comply with the generative AI policy. If you did not use any generative AI tools, simply write none below."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": false
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
